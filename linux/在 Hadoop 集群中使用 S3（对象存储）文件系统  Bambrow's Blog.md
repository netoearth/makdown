本文简要介绍了在 Hadoop 集群（包括 Hadoop、Hive 与 Spark）中使用 S3（对象存储）文件系统的方法与注意事项。

## 对象存储

S3（Simple Storage Service）是一种对象存储服务，具有可扩展性、数据可用性、安全性和性能等优势。S3 兼容存储是一种存储解决方案，允许用户访问和管理它通过 S3 兼容接口存储的数据。

现有的主流 S3 存储服务：

-   [AWS S3](https://aws.amazon.com/cn/s3/)
-   [Minio](https://min.io/)
-   [Ceph](https://ceph.com/en/)

### 依赖包位置

Hadoop 自带 S3 依赖，位置如下：

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br></pre></td><td><pre><span><span>$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-aws-3.1.3.jar</span><br><span><span>$HADOOP_HOME</span>/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.271.jar</span><br></pre></td></tr></tbody></table>

但是这些依赖包默认不在 `hadoop classpath` 下面。可以使用以下两种方法引入这两个包：

1.  在 `hadoop-env.sh` 中加入 `export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/tools/lib/*`。更改完毕后可以使用 `hadoop classpath` 确定。
2.  通过软链接：`ln -s $HADOOP_HOME/share/hadoop/tools/lib/*aws* $HADOOP_HOME/share/hadoop/common/lib/`

注意，Minio 可能会出现 `hdfs dfs -ls` 空文件夹报错的情况，也有可能可以正常部署但是无法通过 YARN 提交任务。遇到这种情况可以考虑降低 `aws-java-sdk-bundle` 的版本为 `1.11.84`，然后重启集群。经测试，Hadoop 3.1.3 与 3.2.0 两个版本可以与 `aws-java-sdk-bundle-1.11.84` 兼容，Hadoop 3.3.x 版本暂时无法兼容。更换 `aws-java-sdk-bundle` 包版本后需要分发到所有集群。

### 配置 `core-site.xml`

在 `$HADOOP_HOME/etc/hadoop/core-site.xml` 加入如下内容（分发至所有机器）：

core-site.xml

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></pre></td><td><pre><span><span>&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span><span>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span><span>&lt;<span>configuration</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.access.key<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>[ak]<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.secret.key<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>[sk]<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.connection.ssl.enabled<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>false<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.path.style.access<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>true<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.endpoint<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>http://[endpoint]<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.impl<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>org.apache.hadoop.fs.s3a.S3AFileSystem<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span><span>&lt;/<span>configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table>

更改完毕并重启集群后，可以使用 `hdfs dfs -ls s3a://[bucket]/` 等命令操作 S3 中的文件。

注意，若想将默认文件系统改为 S3，需要配置 `fs.defaultFS` 为 S3 中的一个存储桶。示例如下：

core-site.xml

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></pre></td><td><pre><span><span>&lt;<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>name</span>&gt;</span>fs.defaultFS<span>&lt;/<span>name</span>&gt;</span></span><br><span>  <span>&lt;<span>value</span>&gt;</span>s3a://[default-bucket]/<span>&lt;/<span>value</span>&gt;</span></span><br><span><span>&lt;/<span>property</span>&gt;</span></span><br></pre></td></tr></tbody></table>

注意，使用 Ceph 部署时，会出现 `hdfs dfs -ls` 出现 `listStatus` 报错的情况，该报错会显示 `com.amazonaws.services.s3.model.AmazonS3Exception: Invalid Argument`。这是因为 Ceph 最多只能支持 `max-keys` 为 1000，但在 `core-default.xml` 里该默认设置为 5000。需要增加如下配置来解决：

core-site.xml

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></pre></td><td><pre><span><span>&lt;<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>name</span>&gt;</span>fs.s3a.paging.maximum<span>&lt;/<span>name</span>&gt;</span></span><br><span>  <span>&lt;<span>value</span>&gt;</span>1000<span>&lt;/<span>value</span>&gt;</span></span><br><span><span>&lt;/<span>property</span>&gt;</span></span><br></pre></td></tr></tbody></table>

### 创建 S3 存储桶

有些 S3 实现自带 UI，对 S3 存储桶与文件的操作可以直接在 UI 上完成。也可以使用 [S3 Browser](https://s3browser.com/)（仅限 Windows）或者 [Rclone](https://rclone.org/)、[s3cmd](https://s3tools.org/s3cmd) 等命令行工具。这里简要演示 `s3cmd` 的使用方法：

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br></pre></td><td><pre><span>yum install -y s3cmd</span><br><span>vim ~/.s3cfg </span><br><span>s3cmd mb s3://[some-bucket]</span><br></pre></td></tr></tbody></table>

其中 `~/.s3cfg` 的内容如下（根据自己的情况配置）：

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></pre></td><td><pre><span>host_base = [endpoint]</span><br><span>host_bucket = [endpoint]</span><br><span>bucket_location = us-east-1</span><br><span>use_https = False</span><br><span></span><br><span>access_key = [ak]</span><br><span>secret_key = [sk]</span><br><span></span><br><span>signature_v2 = False</span><br></pre></td></tr></tbody></table>

### 其他配置

至此，已经可以使用 HDFS Client 查看和获取到 S3 的文件了。如需使用 YARN 和 MapReduce，可以尝试增加如下配置：

hdfs-site.xml

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br></pre></td><td><pre><span><span>&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span><span>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span><span>&lt;<span>configuration</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>dfs.namenode.datanode.registration.ip-hostname-check<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>false<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span><span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>dfs.permissions.enabled<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>false<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>dfs.namenode.rpc-bind-host<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>0.0.0.0<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>dfs.namenode.servicerpc-bind-host<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>0.0.0.0<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>dfs.namenode.http-bind-host<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>0.0.0.0<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>dfs.namenode.https-bind-host<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>0.0.0.0<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>dfs.client.use.datanode.hostname<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>false<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>dfs.datanode.use.datanode.hostname<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>false<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span><span>&lt;/<span>configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table>

需要将下面 `yarn-site.xml` 里的 `yarn.resourcemanager.hostname` 改为对应 ResourceManager 的地址：

yarn-site.xml

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br><span>52</span><br><span>53</span><br><span>54</span><br><span>55</span><br><span>56</span><br><span>57</span><br><span>58</span><br><span>59</span><br><span>60</span><br><span>61</span><br><span>62</span><br><span>63</span><br><span>64</span><br><span>65</span><br><span>66</span><br><span>67</span><br><span>68</span><br><span>69</span><br><span>70</span><br><span>71</span><br><span>72</span><br><span>73</span><br><span>74</span><br></pre></td><td><pre><span><span>&lt;?xml version="1.0"?&gt;</span></span><br><span><span>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span><span>&lt;<span>configuration</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.resourcemanager.hostname<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>[yarn-rm-hostname]<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.resourcemanager.store.class<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.log-aggregation-enable<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>true<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.nodemanager.aux-services<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>mapreduce_shuffle<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span>  </span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>mapred.map.output.compress.codec<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>mapreduce.map.output.compress<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>true<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>true<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>false<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.resourcemanager.bind-host<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>0.0.0.0<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.nodemanager.bind-host<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>0.0.0.0<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.nodemanager.bind-host<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>0.0.0.0<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.timeline-service.bind-host<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span>0.0.0.0<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>        <span>&lt;<span>name</span>&gt;</span>yarn.application.classpath<span>&lt;/<span>name</span>&gt;</span></span><br><span>        <span>&lt;<span>value</span>&gt;</span></span><br><span>            /usr/local/hadoop/etc/hadoop,</span><br><span>            /usr/local/hadoop/share/hadoop/common/*,</span><br><span>            /usr/local/hadoop/share/hadoop/common/lib/*,</span><br><span>            /usr/local/hadoop/share/hadoop/hdfs/*,</span><br><span>            /usr/local/hadoop/share/hadoop/hdfs/lib/*,</span><br><span>            /usr/local/hadoop/share/hadoop/mapreduce/*,</span><br><span>            /usr/local/hadoop/share/hadoop/mapreduce/lib/*,</span><br><span>            /usr/local/hadoop/share/hadoop/yarn/*,</span><br><span>            /usr/local/hadoop/share/hadoop/yarn/lib/*</span><br><span>        <span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span><span>&lt;/<span>configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table>

mapred-site.xml

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br></pre></td><td><pre><span><span>&lt;?xml version="1.0"?&gt;</span></span><br><span><span>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span><span>&lt;<span>configuration</span>&gt;</span></span><br><span>    <span>&lt;<span>property</span>&gt;</span></span><br><span>      <span>&lt;<span>name</span>&gt;</span>mapreduce.framework.name<span>&lt;/<span>name</span>&gt;</span></span><br><span>      <span>&lt;<span>value</span>&gt;</span>yarn<span>&lt;/<span>value</span>&gt;</span></span><br><span>    <span>&lt;/<span>property</span>&gt;</span></span><br><span><span>&lt;/<span>configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table>

配置完毕后，需要重启整个 Hadoop 集群。

## Hive 配置步骤

### 依赖包配置

在 Hive 的安装目录下进行以下操作，增加依赖包：

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br></pre></td><td><pre><span>mkdir <span>$HIVE_HOME</span>/auxlib</span><br><span>ln -s <span>$HADOOP_HOME</span>/share/hadoop/tools/lib/*aws* <span>$HIVE_HOME</span>/auxlib/</span><br></pre></td></tr></tbody></table>

### 创建 `core-site.xml`

在 `$HIVE_HOME/conf` 目录下新建 `core-site.xml` 并加入如下配置：

core-site.xml

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br></pre></td><td><pre><span><span>&lt;<span>configuration</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.defaultFS<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>s3a://[default-bucket]<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.access.key<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>[ak]<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.secret.key<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>[sk]<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.connection.ssl.enabled<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>false<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.path.style.access<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>true<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.endpoint<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>http://[endpoint]<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span>  <span>&lt;<span>property</span>&gt;</span></span><br><span>    <span>&lt;<span>name</span>&gt;</span>fs.s3a.impl<span>&lt;/<span>name</span>&gt;</span></span><br><span>    <span>&lt;<span>value</span>&gt;</span>org.apache.hadoop.fs.s3a.S3AFileSystem<span>&lt;/<span>value</span>&gt;</span></span><br><span>  <span>&lt;/<span>property</span>&gt;</span></span><br><span><span>&lt;/<span>configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table>

其中 `fs.defaultFS` 这一项依据 `$HADOOP_HOME/etc/hadoop/core-site.xml` 是否配置此项而定。

### 配置 `hive-env.sh`

在 `$HIVE_HOME/conf` 目录下运行 `cp hive-env.sh.template hive-env.sh` 后添加如下内容：

hive-env.sh

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br></pre></td><td><pre><span></span><br><span><span>export</span> HIVE_AUX_JARS_PATH=<span>$HIVE_HOME</span>/auxlib</span><br></pre></td></tr></tbody></table>

### 重启 Hive 服务

<table><tbody><tr><td><pre><span>1</span><br></pre></td><td><pre><span><span>$HIVE_HOME</span>/bin/hiveservices.sh restart</span><br></pre></td></tr></tbody></table>

## Spark 配置步骤

运行以下语句并分发至所有机器，随后重启 Spark Standalone 集群：

<table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br></pre></td><td><pre><span>wget https://repo1.maven.org/maven2/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar -P <span>$SPARK_HOME</span>/jars</span><br><span>ln -s <span>$HADOOP_HOME</span>/share/hadoop/tools/lib/*aws* <span>$SPARK_HOME</span>/jars/</span><br><span>mv <span>$SPARK_HOME</span>/jars/guava-14.0.1.jar <span>$SPARK_HOME</span>/jars/guava-14.0.1.jar.bak </span><br></pre></td></tr></tbody></table>

如果将是将 Spark 的任务提交到 YARN，则需要在 `spark-env.sh` 里配置 `YARN_CONF_DIR`：

spark-env.sh

<table><tbody><tr><td><pre><span>1</span><br></pre></td><td><pre><span>YARN_CONF_DIR=<span>$HADOOP_HOME</span>/etc/hadoop</span><br></pre></td></tr></tbody></table>