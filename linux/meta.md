当地时间 2 月 23 日，Meta 举办了一场名为“实验室内部：通过 AI 构建虚拟世界”的活动，在活动中，Meta 首席执行官马克·扎克伯格表示，他已经勾画出构建元宇宙的关键步骤。这也是 Meta 市值狂跌 2000 多亿后，扎克伯格首次对外详细谈及 Meta 在元宇宙方向上的具体布局和规划。

“当人们开始在虚拟世界中与来自不同背景的人一起体验事物时，交流变得尤其重要，”他继续说道。“现在，我们有机会改进互联网并设立一个新标准，让我们所有人都可以相互交流，无论我们说什么语言，或者我们来自哪里，都可以无障碍沟通。如果我们做到了这一点，这就是 AI 将全球各地人们联结在一起的一个例子。”

扎克伯格指出，[Facebook](https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651024286&idx=1&sn=d86ecb30c9a94a042e608515250fad89&chksm=bdbe93cd8ac91adbded5ecd2b9342b599e93d629cc45dbf1a8e51dd0343d2a8b43340d79d1ba&scene=27#wechat_redirect "xxx") 一直在努力开发使全球更多人能够访问互联网的技术，并相信这些努力也将转化为元宇宙。

## Meta 放出多个 AI 大招，剑指元宇宙

### 发布 CAIRaoke 项目：创建虚拟世界所必须的对话式 AI

在虚拟世界中，沟通极为重要，而目前现实世界中的许多对话式 AI 并不足够智能，因此[Meta](https://www.infoq.cn/article/4bJ3I5pibSZNvF8m3AgR "xxx")宣布推出 CAIRaoke 项目，旨在创建虚拟世界所必须的对话式 AI。

Facebook 并不是第一个尝试此类研究的公司。[OpenAI](https://www.infoq.cn/article/mfxhTz8IAtav8CN4oYLR "xxx") 去年展示了一种能够从文本中生成图像的神经网络。但扎克伯格表示，CAIRaoke 项目将成为 Meta 未来的核心。

Meta AI 研发团队表示，他们所开发的这套端到端神经模型，能够以远超现有数字助手的表现提供更私人、与上下文契合更紧密的对话内容。

目前，[CAIRaoke项目](https://ai.facebook.com/blog/project-cairaoke/ "xxx")生成的模型已经在 Portal 产品中使用，接下来的目标是把它跟各类增强现实与虚拟现实设备相集成，以期在未来提供极富沉浸式的多模数字助手交互体验。

Meta AI 研发团队表示：“通过研究，我们发现原有数字助手之所以很难再进一步，瓶颈就在于它们采用的底层架构。虽然这些系统只提供“智能助手”这一项功能，但实际上却依赖于四大独立组件：自然语言理解（NLU）、对话状态跟踪（DST）、对话策略（DP）管理以及自然语言生成（NLG）。以此为基础，开发者还得把这些 AI 系统整合起来，这就导致整体方案难以优化、无法适应新的或者不太熟悉的任务，而且需要耗费大量人力对数据集进行手动注释。”

正因为如此，目前大多数设备上提供的数字助手只能给用户提供非常有限的功能选项、完全理解不了对话背景，而且说出来的总是那些翻来覆去的套话。例如，当我们向助手询问本地天气时，但凡再加上一句简单但不太符合套路的追问，例如“是不是比上周更热”，对方就会陷入懵圈状态。

而由 CAIRaoke 项目创建的模型就不同了，人们可以跟自己的语音助手顺畅交谈，模型能够回顾此前对话中的内容、据此转换主题，理解那些更复杂、更细微的上下文背景。更重要的是，它们甚至能提供前所未有的全新交互方式，例如使用手势。

目前，CAIRaoke 项目创建的模型已经开始在 Meta 的视频通话设备 Portal 上使用，希望帮助用户更轻松地建立并管理提醒功能。例如，您可以快速发出如下指令，模型一次就能听懂：

👩: 帮我在 6：30 设个提醒。

✅ : 您指的是早晨还是晚上？

👩: 晚上，提醒我买鸡蛋。

✅ : 好的，将在明晚 6：30 提醒您买鸡蛋。

虽然还处于早期测试，但研发团队表示发现这套模型的表现确实优于标准方案。在 Portal 上，与原有数字助手相比，新模型在评估提醒信息方面获得了显著改进，能够在一组提醒目标中获得更高的成功率、而且无需增加确认对话次数。

但这还只是此项新技术的初步应用。未来，Meta AI 研发团队认为，未来，CAIRaoke 项目生成的模型能够融入全球数百万用户的日常生活。

### 构建真正的交互式对话 AI

要推进对话式 AI 的发展，一大前提就是摸清问题可能涉及的整个范围。大家可能体会到了近年来 NLU 领域的最新进展，特别是[BART](https://www.infoq.cn/article/B3mSXbsqbspgTUutSdcs "xxx")与[GPT-3](https://www.infoq.cn/article/ggR76eweT3U4hdMGghHY "xxx")，并认为 AI 已经能够理解并生成与人类相似的文本内容。但问题没有这么简单，我们先得把负责理解内容的 AI 跟负责生成交互内容的 AI 区分开来。前者的研究和发展态势确实比较乐观，已经能够从各种输入模式中提取含义，进而实现自动语音识别、图像分类和 NLU 等具体应用。但后者对应的则是我们如何利用自己对真实世界的理解，借技术这一载体同他人开展互动。这里涉及的可是发送文本、语音命令、触觉反馈、显示图像、视频、虚拟头像等要素的复杂组合。

整个行业的研究人员和工程师们一致认为，高质量的对话系统必须依靠由 AI 模型支持的可靠理解层。但也有很多人认为这种交互属于工程问题，而非 AI 问题。因此，只有准确把握真实世界的状态，工程师们才能建立起能够处理这些交互的复杂逻辑。工程方法的特点就是其中的系统工作原理较易理解，而且能够在必要时快速调试。但这一思路在对话式 AI 中表现不佳，也直接导致我们当下能够用上的数字助手往往相当“弱智”、帮不上什么大忙。

### 一种新的统一解决方案

![](https://static001.geekbang.org/infoq/6f/6fda2d9f9499267394ae34723e4751a6.png)

以上示例对话展示出我们希望新一代[数字助手](https://www.infoq.cn/article/LLMF99aOaIg9VQCDXPDq "xxx")所应具备的关键技能：不仅可以提供准确、及时的真实信息，还能实现多模工作（跨视觉与语音）、跨域工作（发送消息并预估抵达时间），同时允许我们摆脱僵化的对话模板、自由主导对话的走向。

目前，[AI助手](https://www.infoq.cn/article/8i5HSOqFTWPrr4iCkZrK "xxx")的实现规范需要四组输入与输出——管道中每个层（NLU、DST、DP 与 NLG）各对应一组。另外，它还要求为每层的输入与输出定义标准。以 NLU 为例，传统对话 AI 系统要求明确定义对话本体（例如各种意图与实体）。

但 CAIRaoke 的模型不同，在神经网络的支持下，它不对会话流做任何限定。而且在这套模型中，只需要使用一组训练数据。

CAIRaoke 项目还大大减少了添加新域所涉及的工作量。在标准规范中，扩展新域要求研究人员按顺序构建并调整各个模块，完成后才能安心训练下一个模块。换句话说，只要 NLU 与 DST 每天都发生变化，我们就永远腾不出手来训练 DP。其中一个组件的变更还可能影响其他组件，迫使我们对所有下游模块进行重新训练。这种紧密的依赖性拖慢了下游模块的迭代速度。但凭借我们的端到端技术，我们消除了各模块对于上游模块的依赖，提高了开发与训练速度，成功以更少的资源与数据投入对其他模型进行调优。

另外，这种新方法也让整个对话过程更加健壮，模型立足一点就能查阅全部信息并做出正确决策。以往，某一组件中的一个小小错误就可能对其他组件产生意想不到且难以解决的影响。例如，基于严格规则的现有数字助手会根据编程设定将“p.m.”这一特定表达后的数字理解成下午 xx 点——但 CAIRaoke 项目使用的则是更先进的预训练语言模型，能够更好地理解上下文、意识到相同表达的背后可能对应的是不同的含义。

最后，CAIRaoke 项目还在面向任务的对话当中引入了 Meta AI 最新对话机器人 Blender Bot 2.0 采用的技术。这意味着使用 CAIRaoke 模型构建而成的助手能够更好地理解用户表达，实时搜索互联网上的知识并转化成对话内容，同时始终保持统一的个性倾向。

当然，对于负责生成自然语言的系统，必须随时关注可能引发的安全与隐私问题。如今，大多数 NLG 组件都有脚本化特性，确保助手不会向用户作出令人反感的回应。而一旦把对话的主导权交给用户，数字助手总会有犯错、或者在交互中冒犯到对方的风险。这个问题已经在业界内达成共识。

所以研发团队决定在 BlenderBot 中内置保护措施，尽可能减少攻击性回复的出现几率。例如，Ray-Ban Storeis 和 Portal 就提供可选语音指令，大家可以随时查看并删除语音命令副本，或者彻底关闭语音存储选项。

原有[NLP系统](https://www.infoq.cn/article/4D5Cc1kBdZ8z35yG6k6X "xxx")中普遍存在的另一个问题就是有种“迷之自信”，即模型会很自信地表达某些并不正确的信息。这对端到端技术也是个巨大挑战，因为模型可能更倾向于根据训练数据在对话中引入或变更实体。举个例子，如果我们让助手“提醒我给 Ankita 打电话”，它往往会自作聪明地把呼叫对象改成“Ankit”，因为 Ankita 这个名字并不太常见。为此，我们使用多种数据增强技术和注意力网络来增加 CAIRaoke 项目的健壮性，同时配合 BlenderBot 2.0 的真实应用来减少“迷之自信”。用语音处理各种日常事务虽然 CAIRaoke 项目模型目前还只能作为 Portal 上的提醒功能，但研发团队希望能尽快把它推广到其他领域，包括为用户提供个性化的购物体验，这样助手才能结合大量聊天内容理解上下文、帮助用户更加从容灵活地驾驭对话流。

可以想见，再过几年，CAIRaoke 项目所代表的技术将成为下一代用户与设备间的交互基础。这类沟通方式将在 VR 与 AR 头显等设备上提供无缝化的导航与交互体验，如同当初触控屏取代实体键盘一样再次颠覆我们的操作习惯。

## Meta 宣布将打造一款通用翻译工具

在此次活动中，不难看出扎克伯格想要在语音翻译和语音助手上面发力的决心。语音翻译为什么成为了 Meta[元宇宙](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247555812&idx=1&sn=8ea2e43f9fb811805758222df0e01b08&chksm=fbeaa32bcc9d2a3dedb69ecbec46cc77cb129f8a88bc3492ad4621c571caacd5ce1f449814f1&scene=27#wechat_redirect "xxx")布局中的重中之重？

对于以英语、汉语或者西班牙语等大语种为母语的朋友们来说，今天的应用程序和网络工具已经基本能够满足大家对于翻译服务的需求。但仍有数十亿人被排除在外，他们的母语较为小众、无法快速理解互联网上的大部分信息，也没法用自己的母语直接跟外面的网络世界开展交流。虽然如今的机器翻译（MT）系统正在快速发展，但其基本设计仍然是从大量文本数据中学习模式。所以对于那些“低资源语言”，也就是训练数据量相对较少的语言、特别是压根不存在标准书写规则的语言，学习根本就无从下手。

消除语言障碍的道路漫长而艰难，但为了能让数十亿人使用自己的母语或首选语言访问信息，这又是一道绕不开的坎。如果能让机器翻译再进一步，不仅能够帮助到那些不会说互联网主要语言的人们，更有可能从根本上改变全球社会建立联系、分享观点的方式。

设想一下，如果使用不同语言的用户们能够通过电话、手表或者智能眼镜进行实时交流；又或者世界上任何地区的民众都能用自己的首选语言观看网络上的多媒体内容，那将是怎样的情景。在不久的未来，当虚拟现实与增强现实等新兴技术将虚拟世界与真实世界融合起来之时，翻译工具将是人们开展日常活动的重要助力——无论是在读书俱乐部中、还是在合作项目上，人们都能轻松愉快地完成常规交流。

为此，Meta AI 公布了一项长期计划，希望构建语言与机器翻译工具、同时将世界上大多数语言涵盖进来。

此项计划具体包含两个项目：其一强调“不忽视任何一种语言”，我们正构建一套新的高级 AI 模型，能够用更少的训练示例掌握一门语言，目标就是用它实现对数百种语言的专家级翻译能力——阿斯图里亚斯语、卢甘达语乃至乌尔都语，一切尽在其中；其二则是打造出通用语音翻译器，我们希望设计出前所未有的新方法，将一种语言的语音实时翻译成另外一种语言，借此支持那些没有标准书写规则的语言、以及书面与口语表达并不统一的语言。

必须承认，要达成这个为全球民众提供通用翻译工具的目标，我们还有很多工作要做。但我相信本文公布的成果已经是探索之旅中的重要一步。我们也将不断分享细节并在时机合适时对代码和模型进行开源，让其他人能够在我们的工作基础上再接再厉，共同为这个意义深远的目标贡献力量。

### 语言翻译中的挑战

如今的 AI 翻译系统面向的就不是全球几千种语言，在设计之初也没有考虑到语音到语音这类实时翻译需求。要想真正为全人类服务，机器翻译研究界需要克服三大核心挑战。我们需要为更多语言搜集更多训练数据，同时探索新的方法以利用相对较少的现存数据克服“低资源”障碍；最后，我们还得找到新的方法来评估并持续改进 AI 翻译系统的性能。

事实上，数据稀缺仍然是在翻译工具中拓展语言支持范围的最大障碍之一。文本[翻译类机器翻译系统](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247526507&idx=2&sn=6233611e461f45319b8ec762176e7484&chksm=fbead1a4cc9d58b27cac59dc3ade6e356f042d8641b65b8c667faa84a75ef889a73b67c5ba66&scene=27#wechat_redirect "xxx")往往需要从数百万条带有注释的数据中学习语言模式，也正因为如此，能够进行高质量翻译的机器翻译系统只能支持在互联网上占主导地位的少数几种语言。要想扩展并支持其他语言，就意味着我们需要获取并使用小众语言那本就不多的训练示例。

至于语音到语音实时翻译，获取数据的难度就更大了。大部分语音机器翻译系统必须使用文本作为中间步骤，即先把一种语言的语音转换为文本、将此文本翻译为目标语言，再把结果输入至文本到语音系统以生成音频。这意味着语音到语音翻译的本质仍然是文本翻译，这不仅限制了其效率，同时也难以匹配口语表达习惯。

语音到语音直接翻译模型则能够翻译那些没有标准书写规则的语言。更重要的是，因为不需要经历语音到文本转换、文本翻译、按目标语言生成语音音频等额外步骤，这类模型的翻译速度更快、效率也更高。

除非能为数千种语言找到合适的训练数据之外，否则现有机器翻译系统根本就无法满足整个国际社会的所有需求。不少机器翻译系统甚至只支持双语互译，代表着每种语言都对应一种单独的模型，例如英语-俄语或者日语-西班牙语。这种方法明显很难扩展至数十种语言组合，当然更不可能支持世界各地使用的所有语言了。想象一下，如果想要支持泰语-老挝语或者尼泊尔语-阿萨姆语这类小众互译需求，前面这种方案会要求我们创建并维护几千种不同的翻译模型。不少专家虽然认为多语言系统可能会成为解决问题的答案，但事实证明，把这么多种语言全部整体到一套能够高效、高性能表达所有语种的多语言模型中将极度困难。

语音到语音实时机器翻译模型不仅面临着与文本翻译模型相同的挑战，还需要额外承受另一个难题——延迟。除非能把语言互译时发生的延迟控制在合理水平，“实时”这个概念才能名副其实。这里的挑战在于，不同语种在表达同样含义时，句子的词序可能存在很大差别。即使是最专业的同声传译人员，给出的结果一般也比初始语音落后约三秒。

以德语中的“Ich möchte alle Sprachen übersetzen”为例，它的西班牙语版本是“Quisiera traducir todos los idiomas”。虽然表达的都是“我想要翻译所有语言”，但把德语实时翻译成英语明显要更困难，因为德语中的动词“translate”出现在句末；而西班牙语跟英语的词序更为相似，所以延迟天然更低。

最后，随着所支持语种的数量不断增加，还需要开发出新的方案以评估当前机器翻译模型的真实效果。已经有资源帮助我们快速评估英语到俄语间的翻译质量，但从阿姆哈拉语到哈萨克语呢？再结合之前提到的，不同语言在训练数据搜索与质量衡量方面的差异，这项任务的难度会再上一个档次。对了，除了评估机器翻译系统的准确性之外，还得保证它能以负责任的方式完成翻译。没错，必须找到方法保证机器翻译系统拥有良好的文化敏感性，而且不致产生或者加剧表达中的偏见。

面对重重阻挠，Meta AI 研发团队介绍了 Meta AI 如何破解这三大挑战。

训练低资源与语音到语音直接翻译系统为了支持低资源语言翻译，同时也为未来更多语种的翻译可能（无论是否拥有丰富的书面或口语训练素材）奠定基础，我们开始探索自动数据集创建技术。其中的方案之一就是 LASER，一款开源工具包，目前已经包含用 28 种不同脚本编写的超过 125 种语言。

LASER 能够将不同语种的句子转换为单一的多语言表达形式。以此为基础，研发团队使用大规模多语言相似性搜索来识别具有相似表达的句子，借此反映不同语种之间可能相同的表义。他们使用 LASER 构建了 ccMatrix 与 ccAligned 等系统，这些已经可以在互联网上查找平行文本。

由于低资源语言的可用数据很少，所以研发团队创建了一种新的师-生训练方法，让 LASER 能够专注于特定的语言子群——例如班图语——借此通过更小的数据集完成学习。如此一来，LASER 就能跨语言实现大规模高效运行。每前进一步，都能覆盖更多语言，努力通过扩展、改进、再扩展的方式实现对数百种语言的挖掘，相信它最终能够扩展到一切具备书写规则的语言当中。最近，研发团队又尝试把 LASER 推向语音处理领域：通过在单一多语言空间中构建语音与文本表达，得以实现从一种语言的语音到另一种语言的文本间的翻译能力——在特定情况下，甚至可以直接完成语音到语音翻译。使用这种方法，已经获得了近 1400 小时的法语、德语、西班牙语和英语校准语音。

文本数据虽然重要，但还不足以构建起能满足每个人需求的翻译工具。考虑到语音翻译基准数据只能支持少数几种主导性语言，所以我们创建了 CoVoST 2，它涵盖 22 种语言、同时能在特定资源条件下支持 36 种语言互译方向。另外，很多语种的现成音频素材也比较稀缺。VoxPopuli 包含 23 种语言、总长 40 万小时的语音，能够为语音识别和语音翻译等语音应用提供大规模的半监督与有监督学习。VoxPopuli 随后还被用于为 128 种语言和语音任务（包括语音翻译）构建起最大规模的开放与通用预训练模型。这套模型能够在 CoVoST 2 数据集上对 21 种语言实现从语音到文本的英语版本翻译，且 BLEU 得分为 7.4。

### 让模型适应多种语言与不同模式

除了为机器翻译系统生成更多训练数据，并把这些素材提供给其他研究人员之外，研发团队还在努力提升模型容量、希望能处理更为广泛的语言间互译任务。传统的机器翻译系统往往只能在单模下支持有限的一组语言。如果模型太小而无法表达多种语言，其性能就可能受到影响，进而导致文本到语音翻译不够准确。但建模方面的创新将帮助我们开拓出新的未来，也许到时候翻译模型可以在多种模式间快速无缝移动，包括跨语种应用下的语音到文本、文本到语音、文本到文本乃至语音到语音等等。

为了提高这套机器翻译模型的性能，研发团队投入巨资建立起容量庞大但仍能高效训练的模型，其中的诀窍就是稀疏门控专家混合（sparsely gated mixture-of-expert）模型。通过增加模型大小并学习自动路由功能，不同的令牌会使用不同的专家容量，让我们能够更好地在高资源与低资源翻译性能间取得平衡。

为了让这套机器翻译模型能将文本翻译范畴扩展到 101 种语言，研发团队建立起第一套不以英语为中心的多语言文本翻译系统。双语系统往往会先把源语言翻译成英语，再把英语结果进一步翻译成目标语言。为了让这类系统在效率与质量方面再进一步，研发团队取消了英语媒介，让不同语言间直接互译。这种方式虽然增加了模型容量，但我们发现多语言模型的翻译质量无法企及之前的定制化双语系统。但最近，我们的多语言翻译系统急起直追，不仅在机器翻译研讨会竞赛上拔得头筹、甚至超越了之前最强的双语模型。

最终的目标是让技术方案更具包容性：它应该能够支持那些纯书写语言和没有标准书写规则的语言。为此，我们正在开发一种语音到语音翻译系统，它在推理过程中不依赖于任何中间文本表达。事实证明，这种方法在速度上优于同时包含独立语音识别、机器翻译与语音合成模型的传统级联系统。通过更高的效率与更简单的架构，这种语音到语音直接翻译系统有望在未来的智能设备（例如 AR 头显）上提供与人类相近的实时翻译能力。最后，为了能够还原不同个体的口语表现力和个性化翻译习惯，研发团队还在努力在生成的翻译音频中包含某些特征，例如语调等。

衡量数百种语言的翻译效果在开发出能够覆盖更多语言的大规模翻译模型之后，新的问题再次摆在面前：该如何确定自己得到了更好的成绩、打造出更好的模型？大规模多语言模型的性能衡量属于历史性难题，而且这一次它还要求我们掌握模型所涵盖的所有语言的实际表达技能——人工测试明显会是一个耗时、费力而且严重缺乏可行性的挑战。

为此研发团队创建了 FLORES-101，世界上第一套涵盖 101 种语言的多语言翻译评估数据集。在它的帮助下，研究人员得以快速测试并改进多语言翻译模型。与现有数据集不同，FLORES-101 允许研究人员通过全部语言互译方向来量化系统性能——而不仅仅是翻译为英语。对于那些拥有几十种官方语言的居民来说，这才是真正能够满足他们需求的翻译解决方案。

沟通能力是人类最基本的能力之一。而无论是印刷术还是视频聊天，技术的进步已经一次又一次改变我们分享观点的方式。当这些技术最终能够同时服务于全球数十亿人时，人们的力量才会真正迸发——他们能够以相似的方式获取信息、以相似的方式与别国的陌生人交换意见，而使用的仍然是自己说了、写了几十年的语言。到那个时候，人们将真正拥有选择语言的权力，得以打破一切当下存在的信息与机会壁垒，共同建立起更加包容、全面互联互通的新世界，这也是为什么 Meta 要在语音翻译上面重点发力的原因。

参考链接：

[https://ai.facebook.com/events/inside-the-lab/](https://ai.facebook.com/events/inside-the-lab/)  

[https://www.engadget.com/meta-wants-to-build-a-universal-language-translator-182519805.html](https://www.engadget.com/meta-wants-to-build-a-universal-language-translator-182519805.html)  

[https://www.reuters.com/technology/metaverse-event-metas-zuckerberg-unveils-work-improve-how-humans-chat-ai-2022-02-23/](https://www.reuters.com/technology/metaverse-event-metas-zuckerberg-unveils-work-improve-how-humans-chat-ai-2022-02-23/)